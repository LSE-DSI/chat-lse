{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Defining nodes and relationships. Creating nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created SIMILAR relationship between chunks 6 and 21\n",
      "Created SIMILAR relationship between chunks 6 and 20\n",
      "Created SIMILAR relationship between chunks 7 and 4\n",
      "Created SIMILAR relationship between chunks 8 and 7\n",
      "Created SIMILAR relationship between chunks 2 and 4\n",
      "Created SIMILAR relationship between chunks 2 and 3\n",
      "Created SIMILAR relationship between chunks 10 and 15\n",
      "Created SIMILAR relationship between chunks 21 and 33\n",
      "Created SIMILAR relationship between chunks 20 and 32\n",
      "Created SIMILAR relationship between chunks 20 and 33\n",
      "Created SIMILAR relationship between chunks 9 and 9\n",
      "Created SIMILAR relationship between chunks 21 and 21\n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import psycopg2\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "# Neo4j connection details\n",
    "NEO4J_URI = 'neo4j+s://77208ee0.databases.neo4j.io'\n",
    "NEO4J_USERNAME = 'neo4j'\n",
    "NEO4J_PASSWORD = 'DGgFsTdjIGI9UKE-QcR8RNmdl6TbgSI4NS-rHegOh_s'\n",
    "\n",
    "# PostgreSQL connection parameters\n",
    "DB_HOST = \"158.143.74.10\"\n",
    "DB_PORT = 5432\n",
    "DB_NAME = \"chatlse\"\n",
    "DB_USER = \"chatlse\"\n",
    "DB_PASSWORD = 'chatlse'\n",
    "\n",
    "# Function to parse embedding strings\n",
    "def parse_embedding(embedding_str):\n",
    "    \"\"\"\n",
    "    Parse embedding string into a NumPy array.\n",
    "    Handles formats like \"[0.1, 0.2, -0.3]\" or \"{0.1, 0.2, -0.3}\".\n",
    "    Returns a zero vector if parsing fails.\n",
    "    \"\"\"\n",
    "    if not embedding_str or embedding_str.strip() == \"\":\n",
    "        return np.zeros(1024)\n",
    "\n",
    "    try:\n",
    "        embedding_str = embedding_str.strip('{}[]')  # Remove brackets if present\n",
    "        embedding_list = [float(x) for x in embedding_str.split(',')]  # Convert to floats\n",
    "        return np.array(embedding_list)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing embedding: {embedding_str}. Error: {e}\")\n",
    "        return np.zeros(1024)  # Default zero vector on failure\n",
    "\n",
    "\n",
    "# Neo4j database class\n",
    "class Neo4jDatabase:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "\n",
    "    def create_chunk_node(self, doc_id, chunk_id, title, url, content, context_embedding, chunk_type):\n",
    "        query = \"\"\"\n",
    "        MERGE (c:Chunk {\n",
    "            doc_id: $doc_id,\n",
    "            chunk_id: $chunk_id\n",
    "        })\n",
    "        SET c.type = $chunk_type,\n",
    "            c.title = $title,\n",
    "            c.url = $url,\n",
    "            c.content = $content,\n",
    "            c.context_embedding = $context_embedding\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            session.run(\n",
    "                query,\n",
    "                chunk_type=chunk_type,\n",
    "                doc_id=doc_id,\n",
    "                chunk_id=chunk_id,\n",
    "                title=title,\n",
    "                url=url,\n",
    "                content=content,\n",
    "                context_embedding=context_embedding,\n",
    "            )\n",
    "\n",
    "    def create_summary_chunk_node(self, doc_id, content, summary):\n",
    "        query = \"\"\"\n",
    "        MERGE (sc:SummaryChunk {\n",
    "            doc_id: $doc_id\n",
    "        })\n",
    "        SET sc.content = $content,\n",
    "            sc.summary = $summary,\n",
    "            sc.type = \"summary_chunk\"\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            session.run(\n",
    "                query,\n",
    "                doc_id=doc_id,\n",
    "                content=content,\n",
    "                summary=summary,\n",
    "            )\n",
    "\n",
    "    def create_similar_relationship(self, doc_id1, chunk_id1, doc_id2, chunk_id2):\n",
    "        \"\"\"\n",
    "        Create a bidirectional SIMILAR relationship between two chunks.\n",
    "        \"\"\"\n",
    "        query = \"\"\"\n",
    "        MATCH (c1:Chunk {doc_id: $doc_id1, chunk_id: $chunk_id1}),\n",
    "            (c2:Chunk {doc_id: $doc_id2, chunk_id: $chunk_id2})\n",
    "        CREATE (c1)-[:SIMILAR]->(c2),\n",
    "            (c2)-[:SIMILAR]->(c1)\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            session.run(query, doc_id1=doc_id1, chunk_id1=chunk_id1, doc_id2=doc_id2, chunk_id2=chunk_id2)\n",
    "\n",
    "\n",
    "    def create_belongs_to_relationship(self, doc_id, chunk_id):\n",
    "        \"\"\"\n",
    "        Create a BELONGS_TO relationship between a Chunk and the corresponding SummaryChunk\n",
    "        from the same document.\n",
    "        \"\"\"\n",
    "        query = \"\"\"\n",
    "        MATCH (chunk:Chunk {doc_id: $doc_id, chunk_id: $chunk_id}),\n",
    "            (summary:SummaryChunk {doc_id: $doc_id})\n",
    "        MERGE (chunk)-[:BELONGS_TO]->(summary)\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            session.run(query, doc_id=doc_id, chunk_id=chunk_id)\n",
    "\n",
    "\n",
    "    def create_next_relationship(self, doc_id, prev_chunk_id, chunk_id):\n",
    "        query = \"\"\"\n",
    "        MATCH (prev:Chunk {doc_id: $doc_id, chunk_id: $prev_chunk_id}),\n",
    "              (current:Chunk {doc_id: $doc_id, chunk_id: $chunk_id})\n",
    "        CREATE (prev)-[:NEXT]->(current)\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            session.run(query, doc_id=doc_id, prev_chunk_id=prev_chunk_id, chunk_id=chunk_id)\n",
    "\n",
    "# Fetch and Create Nodes\n",
    "def fetch_and_create_nodes(db):\n",
    "    \"\"\"\n",
    "    Fetch chunks and summary chunks from PostgreSQL and create corresponding nodes in Neo4j.\n",
    "    Limits: 1000 chunks, 500 summary chunks.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        connection = psycopg2.connect(\n",
    "            host=DB_HOST,\n",
    "            port=DB_PORT,\n",
    "            database=DB_NAME,\n",
    "            user=DB_USER,\n",
    "            password=DB_PASSWORD\n",
    "        )\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Fetch limited chunk data\n",
    "        chunks_query = \"\"\"\n",
    "        SELECT \n",
    "            doc_id, \n",
    "            chunk_id, \n",
    "            url, \n",
    "            title, \n",
    "            content, \n",
    "            context_embeddings \n",
    "        FROM lse_doc\n",
    "        LIMIT 1000;\n",
    "        \"\"\"\n",
    "        cursor.execute(chunks_query)\n",
    "        chunks = cursor.fetchall()\n",
    "\n",
    "        # Create Chunk nodes\n",
    "        for row in chunks:\n",
    "            doc_id, chunk_id, url, title, content, context_embeddings = row\n",
    "\n",
    "            # Parse embedding string into a vector\n",
    "            context_embedding = parse_embedding(context_embeddings)\n",
    "\n",
    "            db.create_chunk_node(\n",
    "                doc_id=doc_id,\n",
    "                chunk_id=chunk_id,\n",
    "                title=title,\n",
    "                url=url,\n",
    "                content=content,\n",
    "                context_embedding=context_embedding.tolist(),\n",
    "                chunk_type=\"chunk\"\n",
    "            )\n",
    "\n",
    "        # Fetch limited summary chunk data\n",
    "        summary_query = \"\"\"\n",
    "        SELECT \n",
    "            doc_id, \n",
    "            content, \n",
    "            summary \n",
    "        FROM doc_summary\n",
    "        LIMIT 500;\n",
    "        \"\"\"\n",
    "        cursor.execute(summary_query)\n",
    "        summaries = cursor.fetchall()\n",
    "\n",
    "        # Create SummaryChunk nodes\n",
    "        for row in summaries:\n",
    "            doc_id, content, summary = row\n",
    "            db.create_summary_chunk_node(\n",
    "                doc_id=doc_id,\n",
    "                content=content,\n",
    "                summary=summary\n",
    "            )\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "    finally:\n",
    "        if connection:\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "\n",
    "# Create Similar Relationships\n",
    "def create_similar_relationships(db, chunks):\n",
    "    \"\"\"\n",
    "    Create SIMILAR relationships only between chunks from different documents.\n",
    "    \"\"\"\n",
    "    chunk_embeddings = []\n",
    "    chunk_doc_ids = []\n",
    "    chunk_ids = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        context_embedding = parse_embedding(chunk[5])\n",
    "        if context_embedding is not None and np.any(context_embedding):  # Exclude invalid or zero vectors\n",
    "            chunk_embeddings.append(context_embedding)\n",
    "            chunk_doc_ids.append(chunk[0])  # Document ID\n",
    "            chunk_ids.append(chunk[1])  # Chunk ID\n",
    "\n",
    "    if chunk_embeddings:\n",
    "        chunk_embeddings = np.array(chunk_embeddings)  # Ensure 2D array\n",
    "        chunk_similarity_matrix = cosine_similarity(chunk_embeddings)\n",
    "        chunk_threshold = 0.99  # Threshold for similarity\n",
    "\n",
    "        for i in range(len(chunk_doc_ids)):\n",
    "            for j in range(i + 1, len(chunk_doc_ids)):\n",
    "                if chunk_doc_ids[i] != chunk_doc_ids[j] and chunk_similarity_matrix[i][j] > chunk_threshold:\n",
    "                    db.create_similar_relationship(\n",
    "                        doc_id1=chunk_doc_ids[i],\n",
    "                        chunk_id1=chunk_ids[i],\n",
    "                        doc_id2=chunk_doc_ids[j],\n",
    "                        chunk_id2=chunk_ids[j],\n",
    "                    )\n",
    "                    print(f\"Created SIMILAR relationship between chunks {chunk_ids[i]} and {chunk_ids[j]}\")\n",
    "    else:\n",
    "        print(\"No valid embeddings found for computing chunk similarities.\")\n",
    "\n",
    "\n",
    "def create_belongs_to_relationships(db, chunks):\n",
    "    \"\"\"\n",
    "    Iterate over chunks and create BELONGS_TO relationships for each chunk in the document\n",
    "    to the corresponding SummaryChunk.\n",
    "    \"\"\"\n",
    "    for chunk in chunks:\n",
    "        doc_id = chunk[0]  # Document ID\n",
    "        chunk_id = chunk[1]  # Chunk ID\n",
    "        db.create_belongs_to_relationship(doc_id, chunk_id)\n",
    "\n",
    "def create_next_relationships(db, chunks):\n",
    "    doc_chunk_map = {}\n",
    "    for chunk in chunks:\n",
    "        doc_id = chunk[0]\n",
    "        chunk_id = chunk[1]\n",
    "        if doc_id not in doc_chunk_map:\n",
    "            doc_chunk_map[doc_id] = []\n",
    "        doc_chunk_map[doc_id].append(chunk_id)\n",
    "\n",
    "    for doc_id, chunk_ids in doc_chunk_map.items():\n",
    "        sorted_chunk_ids = sorted(chunk_ids)\n",
    "        prev_chunk_id = None\n",
    "        for chunk_id in sorted_chunk_ids:\n",
    "            if prev_chunk_id:\n",
    "                db.create_next_relationship(doc_id, prev_chunk_id, chunk_id)\n",
    "            prev_chunk_id = chunk_id\n",
    "\n",
    "# Main Script\n",
    "db = Neo4jDatabase(NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD)\n",
    "chunks = fetch_and_create_nodes(db)\n",
    "create_similar_relationships(db, chunks)\n",
    "create_belongs_to_relationships(db, chunks)\n",
    "create_next_relationships(db, chunks)\n",
    "db.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat-lse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
