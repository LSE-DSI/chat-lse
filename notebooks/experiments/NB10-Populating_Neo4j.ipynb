{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Defining nodes and relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import psycopg2\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Neo4j connection details\n",
    "NEO4J_URI = \"neo4j+s://fc98a293.databases.neo4j.io\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PASSWORD = \"jFgHbgFhvKbp791oQ0SBojwUG5Wwu44ImfBblOnPKTE\"\n",
    "\n",
    "# PostgreSQL connection parameters\n",
    "DB_HOST = \"158.143.74.10\"\n",
    "DB_PORT = 5432\n",
    "DB_NAME = \"chatlse\"\n",
    "DB_USER = \"chatlse\"\n",
    "DB_PASSWORD ='chatlse'\n",
    "\n",
    "\n",
    "# Neo4j database class\n",
    "class Neo4jDatabase:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "\n",
    "    def create_chunk_node(self, doc_id, chunk_id, title, url, content, context_embedding, chunk_type):\n",
    "        query = \"\"\"\n",
    "        CREATE (c:Chunk {\n",
    "            type: $chunk_type,\n",
    "            doc_id: $doc_id,\n",
    "            chunk_id: $chunk_id,\n",
    "            title: $title,\n",
    "            url: $url,\n",
    "            content: $content,\n",
    "            context_embedding: $context_embedding\n",
    "        })\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            session.run(\n",
    "                query,\n",
    "                chunk_type=chunk_type,\n",
    "                doc_id=doc_id,\n",
    "                chunk_id=chunk_id,\n",
    "                title=title,\n",
    "                url=url,\n",
    "                content=content,\n",
    "                context_embedding=context_embedding,\n",
    "            )\n",
    "\n",
    "    def create_summary_chunk_node(self, doc_id, content, summary):\n",
    "        query = \"\"\"\n",
    "        CREATE (sc:SummaryChunk {\n",
    "            type: $chunk_type,\n",
    "            doc_id: $doc_id,\n",
    "            content: $content,\n",
    "            summary: $summary\n",
    "        })\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            session.run(\n",
    "                query,\n",
    "                chunk_type=\"summary_chunk\",\n",
    "                doc_id=doc_id,\n",
    "                content=content,\n",
    "                summary=summary,\n",
    "            )\n",
    "\n",
    "    def create_similar_relationship(self, doc_id1, chunk_id1, doc_id2, chunk_id2):\n",
    "        query = \"\"\"\n",
    "        MATCH (c1:Chunk {doc_id: $doc_id1, chunk_id: $chunk_id1}),\n",
    "              (c2:Chunk {doc_id: $doc_id2, chunk_id: $chunk_id2})\n",
    "        CREATE (c1)-[:SIMILAR]->(c2)\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            session.run(query, doc_id1=doc_id1, chunk_id1=chunk_id1, doc_id2=doc_id2, chunk_id2=chunk_id2)\n",
    "\n",
    "    def create_belongs_to_relationship(self, doc_id, chunk_id):\n",
    "        query = \"\"\"\n",
    "        MATCH (chunk:Chunk {doc_id: $doc_id, chunk_id: $chunk_id}),\n",
    "              (summary:SummaryChunk {doc_id: $doc_id})\n",
    "        CREATE (chunk)-[:BELONGS_TO]->(summary)\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            session.run(query, doc_id=doc_id, chunk_id=chunk_id)\n",
    "\n",
    "    def create_next_relationships(self, doc_id, chunks):\n",
    "        sorted_chunks = sorted(chunks, key=lambda x: x[\"chunk_id\"])  # Order by chunk_id\n",
    "        prev_chunk_id = None\n",
    "        for chunk in sorted_chunks:\n",
    "            if prev_chunk_id is None:  # Start with SummaryChunk\n",
    "                query = \"\"\"\n",
    "                MATCH (summary:SummaryChunk {doc_id: $doc_id}),\n",
    "                      (chunk:Chunk {doc_id: $doc_id, chunk_id: $chunk_id})\n",
    "                CREATE (summary)-[:NEXT]->(chunk)\n",
    "                \"\"\"\n",
    "                with self.driver.session() as session:\n",
    "                    session.run(query, doc_id=doc_id, chunk_id=chunk[\"chunk_id\"])\n",
    "            else:\n",
    "                query = \"\"\"\n",
    "                MATCH (prev:Chunk {doc_id: $doc_id, chunk_id: $prev_chunk_id}),\n",
    "                      (chunk:Chunk {doc_id: $doc_id, chunk_id: $chunk_id})\n",
    "                CREATE (prev)-[:NEXT]->(chunk)\n",
    "                \"\"\"\n",
    "                with self.driver.session() as session:\n",
    "                    session.run(query, doc_id=doc_id, prev_chunk_id=prev_chunk_id, chunk_id=chunk[\"chunk_id\"])\n",
    "            prev_chunk_id = chunk[\"chunk_id\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Fetching and Ingesting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix: Relationship creation functions\n",
    "def create_similar_relationships(chunks):\n",
    "    # Extract embeddings and compute similarity\n",
    "    embeddings = [np.array(chunk[7]) for chunk in chunks if isinstance(chunk[7], list)]\n",
    "    doc_ids = [chunk[1] for chunk in chunks]\n",
    "    chunk_ids = [chunk[2] for chunk in chunks]\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "    threshold = 0.9\n",
    "\n",
    "    for i in range(len(chunks)):\n",
    "        for j in range(i + 1, len(chunks)):\n",
    "            if similarity_matrix[i][j] > threshold:\n",
    "                db.create_similar_relationship(\n",
    "                    doc_id1=doc_ids[i],\n",
    "                    chunk_id1=chunk_ids[i],\n",
    "                    doc_id2=doc_ids[j],\n",
    "                    chunk_id2=chunk_ids[j],\n",
    "                )\n",
    "                print(f\"Created SIMILAR relationship between chunk {chunk_ids[i]} and {chunk_ids[j]}\")\n",
    "\n",
    "def create_belongs_to_relationships(chunks):\n",
    "    doc_chunk_map = {}\n",
    "    for chunk in chunks:\n",
    "        doc_id = chunk[1]\n",
    "        chunk_id = chunk[2]\n",
    "        if doc_id not in doc_chunk_map:\n",
    "            doc_chunk_map[doc_id] = []\n",
    "        doc_chunk_map[doc_id].append(chunk_id)\n",
    "\n",
    "    for doc_id, chunk_ids in doc_chunk_map.items():\n",
    "        for chunk_id in chunk_ids:\n",
    "            db.create_belongs_to_relationship(doc_id, chunk_id)\n",
    "            print(f\"Created BELONGS_TO relationship for chunk {chunk_id} and summary {doc_id}\")\n",
    "\n",
    "def create_next_relationships(chunks):\n",
    "    doc_chunk_map = {}\n",
    "    for chunk in chunks:\n",
    "        doc_id = chunk[1]\n",
    "        chunk_id = chunk[2]\n",
    "        if doc_id not in doc_chunk_map:\n",
    "            doc_chunk_map[doc_id] = []\n",
    "        doc_chunk_map[doc_id].append(chunk_id)\n",
    "\n",
    "    for doc_id, chunk_ids in doc_chunk_map.items():\n",
    "        sorted_chunk_ids = sorted(chunk_ids)  # Ensure chunks are ordered\n",
    "        prev_chunk_id = None\n",
    "        for chunk_id in sorted_chunk_ids:\n",
    "            if prev_chunk_id is None:\n",
    "                print(f\"Starting NEXT relationship from summary to chunk {chunk_id} in doc {doc_id}\")\n",
    "            else:\n",
    "                db.create_next_relationships(doc_id, [\n",
    "                    {\"chunk_id\": prev_chunk_id},\n",
    "                    {\"chunk_id\": chunk_id},\n",
    "                ])\n",
    "                print(f\"Created NEXT relationship between chunk {prev_chunk_id} and chunk {chunk_id}\")\n",
    "            prev_chunk_id = chunk_id\n",
    "\n",
    "# Updated fetch_and_create_nodes to handle missing context_embeddings\n",
    "def fetch_and_create_nodes(limit=100):\n",
    "    try:\n",
    "        connection = psycopg2.connect(\n",
    "            host=DB_HOST,\n",
    "            port=DB_PORT,\n",
    "            database=DB_NAME,\n",
    "            user=DB_USER,\n",
    "            password=DB_PASSWORD\n",
    "        )\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Fetch limited data from lse_doc\n",
    "        chunks_query = f\"\"\"\n",
    "        SELECT \n",
    "            id, \n",
    "            doc_id, \n",
    "            chunk_id, \n",
    "            type, \n",
    "            url, \n",
    "            title, \n",
    "            content, \n",
    "            context_embeddings \n",
    "        FROM lse_doc\n",
    "        LIMIT {limit};\n",
    "        \"\"\"\n",
    "        cursor.execute(chunks_query)\n",
    "        chunks = cursor.fetchall()\n",
    "\n",
    "        # Create Chunk nodes\n",
    "        for row in chunks:\n",
    "            id, doc_id, chunk_id, chunk_type, url, title, content, context_embeddings = row\n",
    "            context_embedding = np.array(context_embeddings) if context_embeddings else np.zeros(1024)\n",
    "            db.create_chunk_node(\n",
    "                doc_id=doc_id,\n",
    "                chunk_id=chunk_id,\n",
    "                title=title,\n",
    "                url=url,\n",
    "                content=content,\n",
    "                context_embedding=context_embedding.tolist(),\n",
    "                chunk_type=\"chunk\"\n",
    "            )\n",
    "\n",
    "        # Fetch data from doc_summary\n",
    "        summary_query = \"\"\"\n",
    "        SELECT \n",
    "            doc_id, \n",
    "            content, \n",
    "            summary \n",
    "        FROM doc_summary;\n",
    "        \"\"\"\n",
    "        cursor.execute(summary_query)\n",
    "        summaries = cursor.fetchall()\n",
    "\n",
    "        # Create SummaryChunk nodes\n",
    "        for row in summaries:\n",
    "            doc_id, content, summary = row\n",
    "            db.create_summary_chunk_node(\n",
    "                doc_id=doc_id,\n",
    "                content=content,\n",
    "                summary=summary\n",
    "            )\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "    finally:\n",
    "        if connection:\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "\n",
    "# Initialize Neo4j and execute workflow\n",
    "db = Neo4jDatabase(NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD)\n",
    "chunks = fetch_and_create_nodes(limit=100)\n",
    "\n",
    "create_similar_relationships(chunks)\n",
    "create_belongs_to_relationships(chunks)\n",
    "create_next_relationships(chunks)\n",
    "\n",
    "db.close()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
