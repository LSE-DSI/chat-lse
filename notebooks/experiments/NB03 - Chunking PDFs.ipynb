{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OBJECTIVE:** Test different ways to chunk PDFs into smaller chunks that can be managed by handled by the algorithms we are using.\n",
    "\n",
    "**AUTHOR:** [Aksh Sabherwal](https://www.github.com/akshsabherwal) (edited by [@jonjoncardoso](https://github.com/jonjoncardoso))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Summarised findings***\n",
    "\n",
    "* I found three viable methods of chunking and two non-viable methods. The working ones are methods 1, 4, and 5\n",
    "* The other two methods rely upon topic modelling/semantic meaning. These did not work because topics/semantic do not vary significantly within a doc \n",
    "* Method 1 is a sliding window method, and proved to be the second most effective method. It has the benefit of being universally applicable to all structures of text as we can determine the token size of the window. However, because of this, we also may encounter issues of hanging sentences \n",
    "* Method 4 is the method from the Google Collab notebook Riya shared. It is similar to sliding window, but instead of token size, the window is based on sentence size, so we might encounter issues where some sentences' tokens exceeds the embedding model's token length.\n",
    "* Method 5 is the best method I found, which uses llama-index's SentenceSplitter() functionality. We set a default chunk token size (512), and the function splits the text so that each chunk has at most 512 tokens. However, an additional benefit is that this function keeps sentences together in one chunk, so we do not have issues of hanging sentences like in method 1\n",
    "* Essentially, method 5 is an improved version of method 4. It has method 4's benefit of chunking based on sentences to prevent hanging sentences, but we are also able to determine a chunk size to ensure we do not exceed token limits. Another big benefit is that the code for using SentenceSplitter() is very clean and simple.\n",
    "* We choose a default chunk size of 512 because the GTE large model's has a maximum intake of 512 tokens\n",
    "\n",
    "* In a bonus method, I combine the ideas of the sliding functionality with the SentenceSplitter(), so that we get the benefits of both. However, the issue that arises is that we get many (many) more total chunks and embeddings (roughly three times the amount). This is a tradeoff that we should discuss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<span style=\"color:red\">NOTE FROM Jon: The code below does not work anymore. It requires code to connect to OpenAI API and to use OpenAI models which we have deleted following the merge of </span>[Issue #21 - [ChatLSE] Explore ways to migrate workflow to run on open source LLMs](https://github.com/LSE-DSI/chat-lse/issues/21) into develop.\n",
    "\n",
    "I started reconciling the old version of the notebook with the current version of our codebase but it was taking a lot of time reverting files and adjusting old functions to make this work. So, I'm leaving the notebook as is, in case we need to refer to it in the future. The key things are Aksh's summarised findings above, which have led to us to move on to the technique in the NB04 notebook.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚙️ **SETUP**\n",
    "\n",
    "- Ensure you are running with the `chat-lse` conda environment. See [README.md](../../README.md) for more information.\n",
    "- Install the packages we will need for this experiment:\n",
    "\n",
    "    ```bash\n",
    "    pip install PyPDF2 nest-asyncio==1.6.0 scikit-learn==1.5.0 spacy==3.7.5 nltk==3.8.1 sentence-transformers\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import spacy\n",
    "import nltk\n",
    "import PyPDF2\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Add ../../fastapi_app package to sys.path\n",
    "sys.path.append(os.path.abspath('../../'))\n",
    "# Reuse the same embedding functions in our project\n",
    "from fastapi_app.embeddings import compute_text_embedding, get_models_max_length\n",
    "from fastapi_app.openai_clients import create_openai_embed_client\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/j.cardoso-\n",
      "[nltk_data]     silva/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK needs additional setup\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Constants**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCS_FOLDER = \"./sample-docs/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utils functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(file_path=DOCS_FOLDER):\n",
    "    # Initialize a variable to hold all the text\n",
    "    all_text = \"\"\n",
    "    \n",
    "    # Open the PDF file\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        # Initialize a PDF reader object\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        \n",
    "        # Iterate through each page in the PDF\n",
    "        for page in pdf_reader.pages:\n",
    "            # Extract text from the page\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                all_text += text  # Append the extracted text to all_text\n",
    "\n",
    "    return all_text\n",
    "\n",
    "def clean_text(text):\n",
    "    # Replace all newline characters with a single space\n",
    "    cleaned_text = re.sub(r'\\n', '', text)\n",
    "    # Replace two or more spaces with a single space\n",
    "    cleaned_text = re.sub(r' {2,}', ' ', cleaned_text)\n",
    "    # Replace a space followed by a period with just a period\n",
    "    cleaned_text = re.sub(r' \\.', '.', cleaned_text)\n",
    "    # Replace a space followed by a comma with just a comma\n",
    "    cleaned_text = re.sub(r' ,', ',', cleaned_text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read and clean PDFs\n",
    "\n",
    " Read PDFs from docs folder and perform necessary cleaning using regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LSE-2030-booklet.pdf',\n",
       " 'ConfidentialityPolicy.pdf',\n",
       " 'Appeals-Regulations.pdf',\n",
       " 'InterruptionPolicy.pdf',\n",
       " 'UG-Student-Handbook-Department-of-International-History-2023-24 (1).pdf',\n",
       " 'In-Course-Financial-Support.pdf',\n",
       " 'BA-BSc-Three-Year-scheme-for-students-from-2018.19.pdf',\n",
       " 'comPro.pdf',\n",
       " 'bsc-handbook-21.22.pdf',\n",
       " 'Formatting-and-binding-your-thesis-2021-22.pdf',\n",
       " 'Exam-Procedures-for-Candidates.pdf',\n",
       " 'MSc-Mark-Frame.pdf',\n",
       " 'Student-Guidance-Deferral.pdf',\n",
       " 'Spring-Exam-Timetable-2024-Final.pdf']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_all_pdfs = [file for file in os.listdir(DOCS_FOLDER)]\n",
    "path_all_pdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all the PDFs into text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c3751f8156e4c2bb2bf53b28e63bf95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 14 documents\n"
     ]
    }
   ],
   "source": [
    "docs = {file: read_pdf(os.path.join(DOCS_FOLDER, file)) for file in tqdm(path_all_pdfs)}\n",
    "\n",
    "print(f\"Read {len(docs)} documents\")\n",
    "# Uncomment lines below if you want to see the content of the documents\n",
    "# from pprint import pprint\n",
    "# pprint(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `cleaned_docs` dictionary with a cleaned version of the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c868015502d49dc9d7491f625ffa22e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cleaned_docs= {file: clean_text(doc) for file, doc in tqdm(docs.items())}\n",
    "\n",
    "# Uncomment lines below if you want to see the cleaned text\n",
    "# from pprint import pprint\n",
    "# pprint(cleaned_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of curiosity: how many lines are in each document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UG-Student-Handbook-Department-of-International-History-2023-24 (1).pdf    3327\n",
       "bsc-handbook-21.22.pdf                                                     1629\n",
       "Spring-Exam-Timetable-2024-Final.pdf                                        999\n",
       "Exam-Procedures-for-Candidates.pdf                                          648\n",
       "comPro.pdf                                                                  455\n",
       "Appeals-Regulations.pdf                                                     316\n",
       "In-Course-Financial-Support.pdf                                             306\n",
       "LSE-2030-booklet.pdf                                                        180\n",
       "InterruptionPolicy.pdf                                                      162\n",
       "BA-BSc-Three-Year-scheme-for-students-from-2018.19.pdf                      126\n",
       "ConfidentialityPolicy.pdf                                                   125\n",
       "Student-Guidance-Deferral.pdf                                               109\n",
       "Formatting-and-binding-your-thesis-2021-22.pdf                               98\n",
       "MSc-Mark-Frame.pdf                                                           43\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many lines are in each text?\n",
    "pd.Series(docs).apply(lambda x: len(x.split('\\n'))).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many words, approximately, are in each document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UG-Student-Handbook-Department-of-International-History-2023-24 (1).pdf    28918\n",
       "bsc-handbook-21.22.pdf                                                     15573\n",
       "Exam-Procedures-for-Candidates.pdf                                          8954\n",
       "Spring-Exam-Timetable-2024-Final.pdf                                        6411\n",
       "comPro.pdf                                                                  4091\n",
       "In-Course-Financial-Support.pdf                                             3579\n",
       "Appeals-Regulations.pdf                                                     3113\n",
       "InterruptionPolicy.pdf                                                      1718\n",
       "BA-BSc-Three-Year-scheme-for-students-from-2018.19.pdf                      1398\n",
       "Student-Guidance-Deferral.pdf                                               1270\n",
       "LSE-2030-booklet.pdf                                                        1264\n",
       "Formatting-and-binding-your-thesis-2021-22.pdf                               687\n",
       "ConfidentialityPolicy.pdf                                                    672\n",
       "MSc-Mark-Frame.pdf                                                           337\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many words are in each text? (just to get a sense for the possible number of tokens)\n",
    "pd.Series(cleaned_docs).apply(lambda x: len(x.split())).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Begin investigations...\n",
    "\n",
    "If you were to run the commented code below, you will clearly find that the current texts are too long... we need to try to find a way to chunk them, but in a way that they are usable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished name='Task-5' coro=<main() done, defined at /var/folders/4v/s2gcwfv92pn056z_f5np21dr0000gp/T/ipykernel_66715/1728115547.py:1> exception=KeyError('thenlper/gte-large')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/4v/s2gcwfv92pn056z_f5np21dr0000gp/T/ipykernel_66715/1728115547.py\", line 11, in main\n",
      "    embedding = await compute_text_embedding(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/j.cardoso-silva/Workspace/chat-lse/fastapi_app/embeddings.py\", line 32, in compute_text_embedding\n",
      "    dimensions_args: ExtraArgs = {\"dimensions\": embedding_dimensions} if SUPPORTED_DIMENSIONS_MODEL[embed_model] else {}\n",
      "                                                                         ~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^\n",
      "KeyError: 'thenlper/gte-large'\n"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    # Create the OpenAI embedding client\n",
    "    # Even though we're using the OpenAI library, we're using by default, 'thenlper/gte-large' as the embedding model\n",
    "    # We're using the client just to load the model and tokenizer\n",
    "    client, model, dimensions = await create_openai_embed_client()\n",
    "    dimensions = int(dimensions) if dimensions else 1536  # Default dimensions if not set\n",
    "\n",
    "    # Iterate over texts and compute their embeddings\n",
    "    embeddings = []\n",
    "    for text in cleaned_docs.values():\n",
    "        embedding = await compute_text_embedding(\n",
    "            q=text,\n",
    "            openai_client=client,\n",
    "            embed_model=model,\n",
    "            embedding_dimensions=dimensions\n",
    "        )\n",
    "        embeddings.append(embedding)\n",
    "    \n",
    "    # Example of how to use embeddings (here we just print them)\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        print(f\"Embedding for Text {i+1}: {embedding}\")\n",
    "\n",
    "# Run the asynchronous main function\n",
    "loop = asyncio.get_event_loop()\n",
    "if loop.is_running():\n",
    "    # Reuse the existing running loop in Jupyter Notebook\n",
    "    tasks = asyncio.ensure_future(main())  # Schedule main to run\n",
    "    # You may run tasks.result() in another cell to get the result if needed\n",
    "else:\n",
    "    # If somehow the loop is not running, use asyncio.run (unlikely in Jupyter)\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Chunking methods\n",
    "\n",
    "I found three viable methods of chunking and two non-viable methods. (The working ones are methods 1, 4, and 5)\n",
    "\n",
    "Methods 2 and 5 rely upon topic modelling/semantic meaning. These did not work because topics/semantic do not vary significantly within a doc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Method 1: Sliding window approach\n",
    "\n",
    "I do not think sentence-based chunking is the best way to calculate embeddings for this context ... there will be issues with scalability and more importantly lost context. \n",
    "\n",
    "For this reason, I think it will be worth it to try a __sliding window method__, where we establish a \"window\" of a certain length upon which the embeddings will be calculated and a step size for the window. This will likely be a good idea because a majority of LSE documents do not have a uniform structure, but this method allows us to maintain a contextual link between each chunk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some useful functions for chunking and embedding:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "def sliding_window(text, max_length=6500, step_size=3000):\n",
    "    tokens = text.split()  # Simple whitespace tokenizer\n",
    "    chunks = []\n",
    "\n",
    "    for i in range(0, len(tokens), step_size):\n",
    "        chunk = tokens[i : i + max_length]\n",
    "        chunk_text = \" \".join(chunk)\n",
    "        chunks.append(chunk_text)\n",
    "        if i + max_length >= len(tokens):\n",
    "            break\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "async def chunk_and_embed(file, clean_text, max_length=6500, step_size=3000, verbose=True):\n",
    "    text_chunks = sliding_window(clean_text, max_length, step_size)\n",
    "    if verbose:\n",
    "        # Print the size of each chunk\n",
    "        print(f\"Document: {file}\")\n",
    "        print(f\"Number of chunks: {len(text_chunks)}\")\n",
    "        print(f\"Size of each chunk: {[len(chunk.split()) for chunk in text_chunks]}\")\n",
    "\n",
    "    iterator_description = f\"Computing embeddings for chunks of \\'{file[0:10] + '...' + file[-10:] if len(file) > 20 else file}\\'\"\n",
    "\n",
    "    embeddings = [\n",
    "        await compute_text_embedding(q=chunk, max_length=10)\n",
    "        for chunk in tqdm(text_chunks, desc=iterator_description)\n",
    "    ]\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "async def chunk_and_embed_all(cleaned_docs, max_length=6500, step_size=3000, verbose=True):\n",
    "    embeddings_store = {}\n",
    "    for file, clean_text in cleaned_docs.items():\n",
    "        embeddings = await chunk_and_embed(file, clean_text, max_length, step_size, verbose)\n",
    "        embeddings_store[file] = embeddings\n",
    "    return embeddings_store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me use the largest documents to test this method.\n",
    "\n",
    "- `UG-Student-Handbook-Department-of-International-History-2023-24 (1).pdf`\n",
    "- `bsc-handbook-21.22.pdf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "handbook_IH = 'UG-Student-Handbook-Department-of-International-History-2023-24 (1).pdf'\n",
    "handbook_EC = 'bsc-handbook-21.22.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: UG-Student-Handbook-Department-of-International-History-2023-24 (1).pdf\n",
      "Number of chunks: 1\n",
      "Size of each chunk: [28918]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d561986fc72438b9cb08b0c1a83911d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing embeddings for chunks of 'UG-Student...24 (1).pdf':   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_embeddings = await chunk_and_embed(handbook_IH, cleaned_docs[handbook_IH], max_length=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: bsc-handbook-21.22.pdf\n",
      "Number of chunks: 5\n",
      "Size of each chunk: [6500, 6500, 6500, 6500, 3573]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "904426813c7f437db280c868d1e2a484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing embeddings for chunks of 'bsc-handbo...-21.22.pdf':   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/j.cardoso-silva/miniconda3/envs/chat-lse/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "example_embeddings = await chunk_and_embed(handbook_EC, cleaned_docs[handbook_EC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text_id, embeddings in embeddings_store.items():\n",
    "    print(f\"Embedding for Text {text_id}:\")\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        print(f\"  Chunk {i+1}: {embedding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it worked for the \"Appeals Regulation\" text... let's try it for the other ones as well "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nest_asyncio\n",
    "\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "# def sliding_window(text, max_length=6500, step_size=3000):\n",
    "#     tokens = text.split()  # Simple whitespace tokenizer\n",
    "#     chunks = []\n",
    "\n",
    "#     for i in range(0, len(tokens), step_size):\n",
    "#         chunk = tokens[i:i + max_length]\n",
    "#         chunk_text = \" \".join(chunk)\n",
    "#         chunks.append(chunk_text)\n",
    "#         if i + max_length >= len(tokens):\n",
    "#             break\n",
    "    \n",
    "#     return chunks\n",
    "\n",
    "# embeddings_store = {}\n",
    "\n",
    "# async def main():\n",
    "#     client, model, dimensions = await create_openai_embed_client()\n",
    "#     dimensions = int(dimensions) if dimensions else 1536\n",
    "\n",
    "#     text_id = 1  # A simple counter or identifier for each text\n",
    "#     for text in cleaned_texts:\n",
    "#         text_chunks = sliding_window(text)\n",
    "#         embeddings = []\n",
    "#         for chunk in text_chunks:\n",
    "#             embedding = await compute_text_embedding(\n",
    "#                 q=chunk,\n",
    "#                 openai_client=client,\n",
    "#                 embed_model=model,\n",
    "#                 embedding_dimensions=dimensions\n",
    "#             )\n",
    "#             embeddings.append(embedding)\n",
    "#         embeddings_store[text_id] = embeddings\n",
    "#         text_id += 1\n",
    "\n",
    "#     for text_id, embeddings in embeddings_store.items():\n",
    "#         print(f\"Embedding for Text {text_id}:\")\n",
    "#         for i, embedding in enumerate(embeddings):\n",
    "#             print(f\"  Chunk {i+1}: {embedding}\")\n",
    "\n",
    "# await main()\n",
    "\n",
    "# embeddings_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error suggests that one of the texts still has 15001 tokens, which is strange since we specified that the limit of the window is 6500..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have found out that actually the method I have instated for splitting based on chunking makes the crucial and incorrect assumption that one character equals one token, which is not the case for many OpenAI models... Let's use a byte-pair encoding (BPE) tokenizer instead to more accurately chunk based on the number of tokens. \n",
    "\n",
    "This is not entirely accurate either though to estimate the number of tokens taken in by this specific embeddings model, so we need to use a smaller window size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "def sliding_window(text, max_length=8000, step_size=3000):\n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), step_size):\n",
    "        chunk = tokens[i:i + max_length]\n",
    "        chunk_text = tokenizer.convert_tokens_to_string(chunk)\n",
    "        chunks.append(chunk_text)\n",
    "        if i + max_length >= len(tokens):\n",
    "            break\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "embeddings_store = {}\n",
    "\n",
    "async def main():\n",
    "    client, model, dimensions = await create_openai_embed_client()\n",
    "    dimensions = int(dimensions) if dimensions else 1536\n",
    "\n",
    "    text_id = 1  # A simple counter or identifier for each text\n",
    "    for text in cleaned_texts:\n",
    "        text_chunks = sliding_window(text)\n",
    "        embeddings = []\n",
    "        for chunk in text_chunks:\n",
    "            embedding = await compute_text_embedding(\n",
    "                q=chunk,\n",
    "                openai_client=client,\n",
    "                embed_model=model,\n",
    "                embedding_dimensions=dimensions\n",
    "            )\n",
    "            embeddings.append(embedding)\n",
    "        embeddings_store[text_id] = embeddings\n",
    "        text_id += 1\n",
    "\n",
    "    for text_id, embeddings in embeddings_store.items():\n",
    "        print(f\"Embedding for Text {text_id}:\")\n",
    "        for i, embedding in enumerate(embeddings):\n",
    "            print(f\"  Chunk {i+1}: {embedding}\")\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I had initially encountered an issue where the embeddings for the first chunk of text 4 was just a vector with all entries being null. I fixed this by adding more settings to the regex function at the start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Method 2: Topic modelling (incompatible with our purposes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way we could try is by determining what the key topics are in each text and chunking based on that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Assuming cleaned_texts is a list of document strings\n",
    "for index, text in enumerate(cleaned_texts):\n",
    "    document = [text]\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(document)\n",
    "\n",
    "    # Step 2: Apply LDA\n",
    "    lda = LatentDirichletAllocation(n_components=2, random_state=0)\n",
    "    lda.fit(X)\n",
    "\n",
    "    # Heading for the output\n",
    "    print(f\"\\nText {index + 1} Analysis:\")  # Dynamic heading for each text\n",
    "\n",
    "    # Step 3: View topics (each topic as a list of words)\n",
    "    def print_topics(model, vectorizer, top_n=20):\n",
    "        for idx, topic in enumerate(model.components_):\n",
    "            print(f\"Topic {idx + 1}\")\n",
    "            print([(vectorizer.get_feature_names_out()[i], topic[i]) for i in topic.argsort()[:-top_n - 1:-1]])\n",
    "\n",
    "    print_topics(lda, vectorizer)\n",
    "\n",
    "    # Example of assigning topics to new documents\n",
    "    doc_topic_dist = lda.transform(X)\n",
    "    print(\"\\nDocument topic distribution:\")\n",
    "    print(doc_topic_dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(model, vectorizer, top_n=20):\n",
    "    topic_keywords = []\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        keywords = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-top_n - 1:-1]]\n",
    "        topic_keywords.append(keywords)\n",
    "    return topic_keywords\n",
    "\n",
    "def chunk_text_by_keywords(text, keywords):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    # Initialize chunks (a list of lists, each list is a chunk of sentences)\n",
    "    chunks = [[] for _ in keywords]\n",
    "\n",
    "    # Assign sentences to the chunk of the topic they are most relevant to\n",
    "    for sentence in sentences:\n",
    "        # Count keyword occurrences in the sentence\n",
    "        keyword_counts = [sum(sentence.count(keyword) for keyword in topic_keywords) for topic_keywords in keywords]\n",
    "        # Find the topic with the maximum count of keywords\n",
    "        max_topic = keyword_counts.index(max(keyword_counts))\n",
    "        # Append the sentence to the corresponding chunk\n",
    "        chunks[max_topic].append(sentence)\n",
    "\n",
    "    # Join sentences back to form coherent chunks\n",
    "    return [\" \".join(chunk) for chunk in chunks]\n",
    "\n",
    "for index, text in enumerate(cleaned_texts):\n",
    "    document = [text]\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(document)\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components=2, random_state=0)\n",
    "    lda.fit(X)\n",
    "\n",
    "    # Extract keywords for the current text's topics\n",
    "    keywords = extract_keywords(lda, vectorizer)\n",
    "    print(f\"\\nText {index + 1} Keywords and Chunks:\")\n",
    "    for i, topic_keywords in enumerate(keywords):\n",
    "        print(f\"Topic {i + 1} Keywords: {topic_keywords}\")\n",
    "\n",
    "    # Chunk the text based on these keywords\n",
    "    text_chunks = chunk_text_by_keywords(text, keywords)\n",
    "    for i, chunk in enumerate(text_chunks):\n",
    "        print(f\"Chunk {i + 1} for Text {index + 1}: {chunk}...\")  # Print the first 100 characters of each chunk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue does seem to be that we have insufficient content in each document for topic modelling... this could be the reason why the keywords are the same in topic 1 and topic 2, and so we only get 1 chunk for each text. For this reason I do not think this is a viable method for chunking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Method 3: Semantic similarity-based splitting (incompatible with our purposes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I input a prompt into the chat interface, I would ideally want to be returned with sections that are directly relevant to my question. So, I think it would be worth trying ways to split sections of the docs semantically. It is similar to method 2, except we would be considering more semantic aspects rather than topic modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].mean(dim=0)\n",
    "\n",
    "\n",
    "def alt_clean_text(text):\n",
    "    # Replace two or more spaces with a single space\n",
    "    cleaned_text = re.sub(r' {2,}', ' ', text)\n",
    "    # Replace a space followed by a period with just a period\n",
    "    cleaned_text = re.sub(r' \\.', '.', text)\n",
    "    # Replace a space followed by a comma with just a comma\n",
    "    cleaned_text = re.sub(r' ,', ',', text)\n",
    "    return cleaned_text\n",
    "\n",
    "alt_cleaned_texts = [alt_clean_text(i) for i in test_texts]\n",
    "\n",
    "# Example text\n",
    "text = alt_cleaned_texts[-2]\n",
    "paragraphs = text.split('\\n\\n')\n",
    "\n",
    "# Compute embeddings\n",
    "embeddings = [get_embedding(para) for para in paragraphs]\n",
    "\n",
    "# Determine breakpoints based on embedding similarity\n",
    "for i in range(1, len(embeddings)):\n",
    "    sim = cosine_similarity([embeddings[i-1].detach().numpy()], [embeddings[i].detach().numpy()])\n",
    "    if sim < 0.999:  # Threshold needs adjustment based on your specific needs\n",
    "        print(f\"Split between paragraph {i-1} and {i} due to low similarity: {sim}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmmmm, even though we have a very high threshold for cosine similarity, we still aren't getting any results of paragraph splitting. Perhaps this is because semantically-speaking, each document is self-contained, and so it would be difficult to differentiate based on this alone... let's try something else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Method 4: D. Brouke's method (template repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Google Colab notebook, they use a method similar to method 1, but they omit the step of a \"sliding window\" and just split chunks into 5,7, or 10 sentences. \n",
    "\n",
    "They also use NLP to handle splitting into sentences, which might be more robust and efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English # see https://spacy.io/usage for install instructions\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Add a sentencizer pipeline, see https://spacy.io/api/sentencizer/ \n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# Create a document instance as an example\n",
    "doc = nlp(\"This is a sentence. This another sentence.\")\n",
    "assert len(list(doc.sents)) == 2\n",
    "\n",
    "# Access the sentences of the document\n",
    "print(list(doc.sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So all the code did is break up the string with two sentences into a \"list\" with two strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import spacy\n",
    "\n",
    "# Load the spaCy language model\n",
    "\n",
    "# Initialize a dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Process each text in the list with its index\n",
    "for index, text in enumerate(tqdm.tqdm(cleaned_texts)):\n",
    "    # Analyze the text with spaCy to get sentences\n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    \n",
    "    # Convert all Sentence objects to strings\n",
    "    sentences = [str(sentence) for sentence in sentences]\n",
    "    \n",
    "    # Use the index as the key for each document's results\n",
    "    results[f\"document_{index}\"] = {\n",
    "        \"sentences\": sentences,\n",
    "        \"sentence_count\": len(sentences)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint \n",
    "\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm as tqdm\n",
    "\n",
    "# Create a list of dictionaries from cleaned texts to ensure compatability with the method prescribed in the Google colab notebook \n",
    "dict_cleaned_texts = [{'text': text} for text in cleaned_texts]\n",
    "\n",
    "for item in tqdm.tqdm(dict_cleaned_texts):\n",
    "    item[\"sentences\"] = list(nlp(item[\"text\"]).sents)\n",
    "    \n",
    "    # Make sure all sentences are strings\n",
    "    item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n",
    "    \n",
    "    # Count the sentences \n",
    "    item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])\n",
    "\n",
    "# Define split size to turn groups of sentences into chunks\n",
    "\n",
    "num_sentence_chunk_size = 5\n",
    "\n",
    "# Create a function that recursively splits a list into desired sizes\n",
    "def split_list(input_list: list, \n",
    "               slice_size: int) -> list[list[str]]:\n",
    "    \"\"\"\n",
    "    Splits the input_list into sublists of size slice_size (or as close as possible).\n",
    "\n",
    "    For example, a list of 17 sentences would be split into two lists of [[10], [7]]\n",
    "    \"\"\"\n",
    "    return [input_list[i:i + slice_size] for i in range(0, len(input_list), slice_size)]\n",
    "\n",
    "# Loop through pages and texts and split sentences into chunks\n",
    "for item in tqdm.tqdm(dict_cleaned_texts):\n",
    "    item[\"sentence_chunks\"] = split_list(input_list=item[\"sentences\"],\n",
    "                                         slice_size=num_sentence_chunk_size)\n",
    "    item[\"num_chunks\"] = len(item[\"sentence_chunks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_cleaned_texts[0][\"sentence_chunks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in dict_cleaned_texts:\n",
    "    # Convert each list of sentences in sentence_chunks to a single concatenated string\n",
    "    item[\"sentence_chunks\"] = [' '.join(chunk) for chunk in item[\"sentence_chunks\"]]\n",
    "\n",
    "print(dict_cleaned_texts[0][\"sentence_chunks\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings import compute_text_embedding  \n",
    "from openai_clients import create_openai_embed_client\n",
    "from transformers import GPT2Tokenizer\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "embeddings_store = {}\n",
    "\n",
    "async def main():\n",
    "    client, model, dimensions = await create_openai_embed_client()\n",
    "    dimensions = int(dimensions) if dimensions else 1536\n",
    "\n",
    "    text_id = 1  # A simple counter or identifier for each text\n",
    "    \n",
    "    # Loop through each dictionary in the list\n",
    "    for item in dict_cleaned_texts:\n",
    "        sentence_chunks = item['sentence_chunks']  # Accessing sentence_chunks from each dictionary\n",
    "        embeddings = []\n",
    "        for chunk in sentence_chunks:\n",
    "            embedding = await compute_text_embedding(\n",
    "                q=chunk,\n",
    "                openai_client=client,\n",
    "                embed_model=model,\n",
    "                embedding_dimensions=dimensions\n",
    "            )\n",
    "            embeddings.append(embedding)\n",
    "        embeddings_store[text_id] = embeddings\n",
    "        text_id += 1\n",
    "\n",
    "    for text_id, embeddings in embeddings_store.items():\n",
    "        print(f\"Embedding for Text {text_id}:\")\n",
    "        for i, embedding in enumerate(embeddings):\n",
    "            print(f\"  Chunk {i+1}: {embedding}\")\n",
    "\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: So far, I have been using the method in the Google Colab notebook. However, I have run into an issue of token length as before, even though I have restricted sentence length to 5. I will need to make use of the sliding window function from before again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from embeddings import compute_text_embedding  \n",
    "from openai_clients import create_openai_embed_client\n",
    "from transformers import GPT2Tokenizer\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "def sliding_window(text, max_length=8000, step_size=3000):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), step_size):\n",
    "        chunk = tokens[i:i + max_length]\n",
    "        chunk_text = tokenizer.convert_tokens_to_string(chunk)\n",
    "        chunks.append(chunk_text)\n",
    "        if i + max_length >= len(tokens):\n",
    "            break\n",
    "    return chunks\n",
    "\n",
    "embeddings_store = {}\n",
    "\n",
    "async def main():\n",
    "    client, model, dimensions = await create_openai_embed_client()\n",
    "    dimensions = int(dimensions) if dimensions else 1536\n",
    "\n",
    "    text_id = 1  # A simple counter or identifier for each text\n",
    "    \n",
    "    # Loop through each dictionary in the list\n",
    "    for item in dict_cleaned_texts:\n",
    "        sentence_chunks = item['sentence_chunks']  # Accessing sentence_chunks from each dictionary\n",
    "        embeddings = []\n",
    "        for chunk in sentence_chunks:\n",
    "            sub_chunks = sliding_window(chunk)  # Use sliding_window to handle long chunks\n",
    "            chunk_embeddings = []\n",
    "            for sub_chunk in sub_chunks:\n",
    "                embedding = await compute_text_embedding(\n",
    "                    q=sub_chunk,\n",
    "                    openai_client=client,\n",
    "                    embed_model=model,\n",
    "                    embedding_dimensions=dimensions\n",
    "                )\n",
    "                chunk_embeddings.append(embedding)\n",
    "            embeddings.append(chunk_embeddings)\n",
    "        embeddings_store[text_id] = embeddings\n",
    "        text_id += 1\n",
    "\n",
    "    for text_id, embeddings in embeddings_store.items():\n",
    "        print(f\"Embedding for Text {text_id}:\")\n",
    "        for i, chunk_embeddings in enumerate(embeddings):\n",
    "            print(f\"  Chunk {i+1}:\")\n",
    "            for sub_i, embedding in enumerate(chunk_embeddings):\n",
    "                print(f\"    Sub-chunk {sub_i+1}: {embedding}\")\n",
    "\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Method 5: llama-index SentenceSplitter function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An issue with tokenisation methods is that we might get the issue of sentences being embedded where certain phrases or words are incomplete. SentenceSplitter resolves that by trying to keep paragraphs and sentences together. In effect, this is a combination of methods 1 and 5, which will help to accomodate for the shortfalls of both methods. The shortfall of method 1 (sliding window) is the idea of hanging sentences; the shortfall of method 4 is token numbers exceeding the model allowance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# Assuming constants and imports are defined elsewhere\n",
    "DEFAULT_CHUNK_SIZE = 500  # This means each chunk has at most 500 tokens\n",
    "SENTENCE_CHUNK_OVERLAP = 50  # Example overlap\n",
    "CHUNKING_REGEX = r\"[^,\\.;]+[,\\.;]?\"  # Simple sentence splitter regex\n",
    "DEFAULT_PARAGRAPH_SEP = \"\\n\\n\"  # Paragraph separator\n",
    "\n",
    "# Import required functions and classes - make sure these are defined\n",
    "# from your_module import get_tokenizer, split_by_sentence_tokenizer, split_by_sep, split_by_regex, split_by_char, CallbackManager, default_id_func\n",
    "\n",
    "class _Split:\n",
    "    def __init__(self, text, is_sentence, token_size):\n",
    "        self.text = text\n",
    "        self.is_sentence = is_sentence\n",
    "        self.token_size = token_size\n",
    "\n",
    "# Now using SentenceSplitter class with necessary modifications for clarity\n",
    "splitter = SentenceSplitter()\n",
    "import nest_asyncio\n",
    "from transformers import GPT2Tokenizer\n",
    "from embeddings import compute_text_embedding  \n",
    "from openai_clients import create_openai_embed_client\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "def sliding_window(text, max_length=8000, step_size=3000):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), step_size):\n",
    "        chunk = tokens[i:i + max_length]\n",
    "        chunk_text = tokenizer.convert_tokens_to_string(chunk)\n",
    "        chunks.append(chunk_text)\n",
    "        if i + max_length >= len(tokens):\n",
    "            break\n",
    "    return chunks\n",
    "\n",
    "embeddings_store = {}\n",
    "\n",
    "async def main():\n",
    "    client, model, dimensions = await create_openai_embed_client()\n",
    "    dimensions = int(dimensions) if dimensions else 1536\n",
    "\n",
    "    text_id = 1  # A simple counter or identifier for each text\n",
    "    \n",
    "    # Assuming 'splitter' is an instance of SentenceSplitter\n",
    "    splitter = SentenceSplitter()  # Initialize this based on your specific implementation\n",
    "\n",
    "    # Loop through each text in cleaned_texts, chunk it, then calculate embeddings\n",
    "    for text in cleaned_texts:\n",
    "        print(f\"\\nProcessing Text {text_id}:\")\n",
    "        sentence_chunks = splitter.split_text(text)  # Use your SentenceSplitter to split the text\n",
    "        embeddings = []\n",
    "        \n",
    "        for i, chunk in enumerate(sentence_chunks):\n",
    "            print(f\"  Chunk {i+1}: {chunk[:100]}...\")  # Print first 100 characters of each chunk for brevity\n",
    "            sub_chunks = sliding_window(chunk)  # Handle long chunks\n",
    "            chunk_embeddings = []\n",
    "            \n",
    "            for j, sub_chunk in enumerate(sub_chunks):\n",
    "                print(f\"    Sub-chunk {j+1}: {sub_chunk[:50]}...\")  # Print first 50 characters of each sub-chunk\n",
    "                embedding = await compute_text_embedding(\n",
    "                    q=sub_chunk,\n",
    "                    openai_client=client,\n",
    "                    embed_model=model,\n",
    "                    embedding_dimensions=dimensions\n",
    "                )\n",
    "                chunk_embeddings.append(embedding)\n",
    "                print(f\"      Embedding for Sub-chunk {j+1}: {embedding[:10]}...\")  # Print first 10 elements of embedding array\n",
    "\n",
    "            embeddings.append(chunk_embeddings)\n",
    "        \n",
    "        embeddings_store[text_id] = embeddings\n",
    "        text_id += 1\n",
    "\n",
    "    print(\"\\nFinished processing all texts.\")\n",
    "\n",
    "# Run the main function in the asyncio event loop\n",
    "await main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Method 6: Misc - testing GTE embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding \n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"thenlper/gte-large\") \n",
    "\n",
    "embeddings = embed_model.get_text_embedding(\"Hello World!\") \n",
    "print(len(embeddings))\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have installed the necessary dependencies and set up necessary objects, we will test the embeddings on cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_CHUNK_SIZE = 512  # Setting as 512 because this is the model's maximum length; any text with longer text is truncated down to 512 tokens. \n",
    "\n",
    "# Initialize the tokenizer and the embedding model\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"thenlper/gte-large\")\n",
    "\n",
    "embeddings_store = {}\n",
    "\n",
    "def main():\n",
    "    text_id = 1  # A simple counter or identifier for each text\n",
    "    \n",
    "    # Assuming 'splitter' is an instance of SentenceSplitter\n",
    "    # Initialize the SentenceSplitter with specific chunk size\n",
    "    splitter = SentenceSplitter(chunk_size=DEFAULT_CHUNK_SIZE, chunk_overlap=SENTENCE_CHUNK_OVERLAP)\n",
    "    # Initialize this based on your specific implementation\n",
    "\n",
    "    # Loop through each text in cleaned_texts, process each sentence, then calculate embeddings\n",
    "    for text in cleaned_texts:\n",
    "        print(f\"\\nProcessing Text {text_id}:\")\n",
    "        sentence_chunks = splitter.split_text(text)  # Use your SentenceSplitter to split the text\n",
    "        embeddings = []\n",
    "        \n",
    "        for i, chunk in enumerate(sentence_chunks):\n",
    "            print(f\"  Chunk {i+1}: {chunk[:100]}...\")  # Print first 100 characters of each chunk for brevity\n",
    "            embedding = embed_model.get_text_embedding(chunk)\n",
    "            embeddings.append(embedding)\n",
    "            print(f\" Embedding for Chunk {i+1}: {embedding[:10]}...\")  # Print first 10 elements of embedding array\n",
    "        \n",
    "        embeddings_store[text_id] = embeddings\n",
    "        text_id += 1\n",
    "\n",
    "    print(\"\\nFinished processing all texts.\")\n",
    "\n",
    "# Run the main function\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentence_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.1 Bonus combined method: Integrating SentenceSplitter() with sliding_window()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using method 5 so far is the best, and will likely be sufficient for our purposes. However, a nice benefit of the sliding window method was that we were able to retain more context between paragraphs by having the \"sliding\" functionality. Let's see if we can find a way to integrate the function so that we get sub-chunks as well as non-hanging sentences...\n",
    "\n",
    "We're not going to be able to use our previous sliding window function, because we will encounter the same issue of hanging sentences. Instead, we can try something where we split sentences into even smaller chunks and then calculate the embeddings for two chunks at the same time. For instance, say we have chunks 1,2,3,4... then we would calculate embeddings for chunks 1 and 2, then 2 and 3, then 3 and 4 and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"thenlper/gte-large\")\n",
    "\n",
    "def create_overlapping_chunks(sentence_chunks):\n",
    "    \"\"\"Create overlapping chunks from list of sentence chunks.\"\"\"\n",
    "    combined_chunks = []\n",
    "    for i in range(len(sentence_chunks) - 1):\n",
    "        # Merge two consecutive chunks\n",
    "        combined_chunk = sentence_chunks[i] + \" \" + sentence_chunks[i + 1]\n",
    "        combined_chunks.append(combined_chunk)\n",
    "    return combined_chunks\n",
    "\n",
    "def compute_embeddings_for_chunks(chunks):\n",
    "    \"\"\"Compute embeddings for each chunk.\"\"\"\n",
    "    embeddings = []\n",
    "    for chunk in chunks:\n",
    "        embedding = embed_model.get_text_embedding(chunk)\n",
    "        embeddings.append(embedding)\n",
    "    return embeddings\n",
    "\n",
    "async def main():\n",
    "    # Assuming 'splitter.split_text(text)' returns a list of sentence chunks\n",
    "    embeddings_store = {}\n",
    "    text_id = 1\n",
    "    splitter = SentenceSplitter(chunk_size=206, chunk_overlap=SENTENCE_CHUNK_OVERLAP)\n",
    "    for text in cleaned_texts:\n",
    "        print(f\"Processing Text {text_id}:\")\n",
    "        sentence_chunks = splitter.split_text(text)  # Split text into sentence chunks\n",
    "        \n",
    "        # Create overlapping chunks from the sentence chunks\n",
    "        overlapping_chunks = create_overlapping_chunks(sentence_chunks)\n",
    "        \n",
    "        # Compute embeddings for each overlapping chunk\n",
    "        embeddings = compute_embeddings_for_chunks(overlapping_chunks)\n",
    "        \n",
    "        embeddings_store[text_id] = embeddings\n",
    "        text_id += 1\n",
    "\n",
    "    # Print or process the embeddings\n",
    "    for text_id, embeddings in embeddings_store.items():\n",
    "        print(f\"Embeddings for Text {text_id}:\")\n",
    "        for i, embedding in enumerate(embeddings):\n",
    "            print(f\"  Embedding {i+1}: {embedding[:10]}...\")  # Show first 10 elements for brevity\n",
    "\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Initialize the TikToken tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def count_tokens(text, tokenizer):\n",
    "    \"\"\"Tokenize the input text using TikToken and return the number of tokens.\"\"\"\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "def create_overlapping_chunks(sentence_chunks):\n",
    "    \"\"\"Create overlapping chunks from list of sentence chunks.\"\"\"\n",
    "    combined_chunks = []\n",
    "    for i in range(len(sentence_chunks) - 1):\n",
    "        # Merge two consecutive chunks\n",
    "        combined_chunk = sentence_chunks[i] + \" \" + sentence_chunks[i + 1]\n",
    "        combined_chunks.append(combined_chunk)\n",
    "    return combined_chunks\n",
    "\n",
    "# Example usage with cleaned_texts assumed to be defined\n",
    "for text in cleaned_texts:\n",
    "    sentence_chunks = splitter.split_text(text)  # Assuming splitter.split_text is adapted for TikToken\n",
    "    overlapping_chunks = create_overlapping_chunks(sentence_chunks)\n",
    "    \n",
    "    # Count tokens using the TikToken tokenizer\n",
    "    for chunk in overlapping_chunks:\n",
    "        print(count_tokens(chunk, tokenizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Method 7: Retrying NLP-based methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def dynamic_chunk(text, max_length=512):\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    doc = nlp(text)\n",
    "    sentences = [sentence.text for sentence in doc.sents]\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_length = len(sentence.split())\n",
    "        if current_length + sentence_length > max_length:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_length = sentence_length\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += sentence_length\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Apply chunking to each text in the list\n",
    "all_chunks = [dynamic_chunk(text) for text in cleaned_texts]\n",
    "\n",
    "# Optionally, print the chunks to verify\n",
    "for text_chunks in all_chunks:\n",
    "    for chunk in text_chunks:\n",
    "        print(chunk)\n",
    "        print(\"\\n---\\n\")  # Separates chunks for readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub_chunk in all_chunks[0]:\n",
    "    print(len(sub_chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings_store = {}\n",
    "text_id = 1\n",
    "sub_chunk_text_id = 1\n",
    "splitter = SentenceSplitter(chunk_size=512, chunk_overlap=SENTENCE_CHUNK_OVERLAP)\n",
    "for chunk in all_chunks:\n",
    "    print(f\"Processing Text {text_id}:\")\n",
    "    for sub_chunk in chunk:\n",
    "        # Compute embeddings for each overlapping chunk\n",
    "        print(f\"Processing Sub-Text {sub_chunk_text_id}:\")\n",
    "        print(f\"Text being processed: {sub_chunk[:100]}\")\n",
    "        embeddings = compute_embeddings_for_chunks(sub_chunk)\n",
    "        embeddings_store[\"sub_chunk_text_id\"] = embeddings\n",
    "        print(f\"Embedding for {sub_chunk_text_id}: {embeddings[:10]}\")\n",
    "        sub_chunk_text_id += 1\n",
    "\n",
    "    text_id += 1\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
